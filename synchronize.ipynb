{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# import scipy.io\n",
    "# import ntplib\n",
    "import datetime\n",
    "# from datetime import datetime, timezone, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "from moviepy.editor import VideoFileClip, concatenate_videoclips\n",
    "import scipy.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = 1\n",
    "walk = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_video_subset(video_path, start_frame, end_frame, output_path):\n",
    "    \"\"\"\n",
    "    Extract a subset of frames from a video file while keeping a subset of the audio.\n",
    "\n",
    "    Parameters:\n",
    "        video_path (str): Path to the input video file.\n",
    "        start_frame (int): Starting frame index.\n",
    "        end_frame (int): Ending frame index.\n",
    "        output_path (str): Path to save the output video file.\n",
    "    \"\"\"\n",
    "    # Load video clip\n",
    "    video_clip = VideoFileClip(video_path)\n",
    "\n",
    "    print(\"video_clip.fps:\", video_clip.fps)\n",
    "\n",
    "    # Extract audio\n",
    "    audio_clip = video_clip.audio\n",
    "\n",
    "    # Extract subset of audio\n",
    "    subset_audio_clip = audio_clip.subclip(start_frame / video_clip.fps, end_frame / video_clip.fps)\n",
    "\n",
    "    # Extract subset of frames\n",
    "    subset_clip = video_clip.subclip(start_frame / video_clip.fps, end_frame / video_clip.fps)\n",
    "\n",
    "    # Set audio for subset clip\n",
    "    subset_clip = subset_clip.set_audio(subset_audio_clip)\n",
    "\n",
    "    # Write video file with audio\n",
    "    subset_clip.write_videofile(output_path, codec=\"libx264\", audio_codec=\"aac\", logger=None)\n",
    "\n",
    "    # Close the video clip\n",
    "    video_clip.close()\n",
    "\n",
    "\n",
    "def extract_video_noaudio_subset(video_path, start_frame, end_frame, output_path):\n",
    "    \"\"\"\n",
    "    Extract a subset of frames from a video file without including audio.\n",
    "\n",
    "    Parameters:\n",
    "        video_path (str): Path to the input video file.\n",
    "        start_frame (int): Starting frame index.\n",
    "        end_frame (int): Ending frame index.\n",
    "        output_path (str): Path to save the output video file.\n",
    "    \"\"\"\n",
    "    # Load video clip\n",
    "    video_clip = VideoFileClip(video_path)\n",
    "\n",
    "    # Extract subset of frames\n",
    "    subset_clip = video_clip.subclip(start_frame / video_clip.fps, end_frame / video_clip.fps)\n",
    "\n",
    "    # Write video file without audio\n",
    "    subset_clip.write_videofile(output_path, codec=\"libx264\", audio=False, logger=None)\n",
    "\n",
    "    # Close the video clip\n",
    "    video_clip.close()\n",
    "\n",
    "def extract_video_audio_subset(video_path, start_frame, end_frame, output_video_path, output_audio_path):\n",
    "    \"\"\"\n",
    "    Extract a subset of frames from a video file and save the video and audio separately.\n",
    "\n",
    "    Parameters:\n",
    "        video_path (str): Path to the input video file.\n",
    "        start_frame (int): Starting frame index.\n",
    "        end_frame (int): Ending frame index.\n",
    "        output_video_path (str): Path to save the output video file.\n",
    "        output_audio_path (str): Path to save the output audio file.\n",
    "    \"\"\"\n",
    "    # Validate input parameters\n",
    "    if not os.path.isfile(video_path):\n",
    "        raise FileNotFoundError(f\"Video file not found at {video_path}\")\n",
    "    if start_frame < 0 or end_frame < 0:\n",
    "        raise ValueError(\"Frame indices must be non-negative\")\n",
    "        # return \"Frame indices must be non-negative\"\n",
    "    if end_frame <= start_frame:\n",
    "        raise ValueError(\"End frame must be greater than start frame\")\n",
    "        # return \"End frame must be greater than start frame\"\n",
    "\n",
    "    # Load video clip\n",
    "    video_clip = VideoFileClip(video_path)\n",
    "\n",
    "    # Extract subset of frames\n",
    "    ## For Walk4\n",
    "    # print(\"subset_clip time:\", start_frame / video_clip.fps, end_frame / video_clip.fps)\n",
    "    # print(\"start frame, end frame:\", start_frame, end_frame)\n",
    "    subset_clip = video_clip.subclip(start_frame / video_clip.fps, end_frame / video_clip.fps)\n",
    "\n",
    "    # Write video file\n",
    "    subset_clip.write_videofile(output_video_path, codec=\"libx264\", logger=None)\n",
    "\n",
    "    # Close the video clip\n",
    "    subset_clip.close()\n",
    "\n",
    "    # Extract audio\n",
    "    audio_clip = video_clip.audio\n",
    "\n",
    "    # Calculate time in seconds for subclipping\n",
    "    start_time = start_frame / video_clip.fps\n",
    "    end_time = end_frame / video_clip.fps\n",
    "\n",
    "    # Extract subset of audio\n",
    "    subset_audio_clip = audio_clip.subclip(start_time, end_time)\n",
    "\n",
    "    # Write audio file\n",
    "    subset_audio_clip.write_audiofile(output_audio_path, logger=None)\n",
    "\n",
    "    # Close the audio clip\n",
    "    subset_audio_clip.close()\n",
    "\n",
    "    # Close the video clip\n",
    "    video_clip.close()\n",
    "\n",
    "\n",
    "def matlab_datenum_to_formatted_string(matlab_datenum):\n",
    "\n",
    "    # Convert MATLAB datenum to Python datenum by subtracting the MATLAB epoch\n",
    "    python_datenum = matlab_datenum\n",
    "\n",
    "    # Convert Python datenum to datetime\n",
    "    python_datetime = datetime.datetime.fromordinal(int(python_datenum)) + datetime.timedelta(days=python_datenum % 1) - datetime.timedelta(days=366)\n",
    "\n",
    "    # Format datetime string\n",
    "    formatted_string = python_datetime.strftime(\"%Y-%m-%d_%H-%M-%S-%f\")[:-3]\n",
    "\n",
    "    return formatted_string\n",
    "\n",
    "\n",
    "def calculate_duration(datetime_str1, datetime_str2):\n",
    "    \"\"\"\n",
    "    Calculate the duration between two datetime strings.\n",
    "\n",
    "    Parameters:\n",
    "        datetime_str1 (str): First datetime string.\n",
    "        datetime_str2 (str): Second datetime string.\n",
    "\n",
    "    Returns:\n",
    "        datetime.timedelta: Duration between the two datetime strings.\n",
    "    \"\"\"\n",
    "    # Convert the datetime strings to datetime objects\n",
    "    datetime_obj1 = datetime.datetime.strptime(datetime_str1, '%Y-%m-%d_%H-%M-%S-%f')\n",
    "    datetime_obj2 = datetime.datetime.strptime(datetime_str2, '%Y-%m-%d_%H-%M-%S-%f')\n",
    "\n",
    "    # Calculate the duration\n",
    "    duration = datetime_obj2 - datetime_obj1\n",
    "\n",
    "    # Return the duration\n",
    "    return duration\n",
    "\n",
    "\n",
    "def find_close_frame(time_label, time_data_array):\n",
    "\n",
    "    frame_index_data = 0\n",
    "    time_label_num = datetime.datetime.strptime(time_label, '%Y-%m-%d_%H-%M-%S-%f')\n",
    "\n",
    "    for temp_index in range(time_data_array.shape[0]):\n",
    "\n",
    "        temp_time_data = datetime.datetime.strptime(time_data_array[temp_index], '%Y-%m-%d_%H-%M-%S-%f')\n",
    "\n",
    "        if temp_time_data >= time_label_num:\n",
    "\n",
    "            frame_index_data = temp_index\n",
    "            # print(\"syncronze frame: \", frame_index_data, time_label, time_data_array[temp_index])\n",
    "            break\n",
    "\n",
    "    return frame_index_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fre_np = 250, fre_gopro = 60, fre_pupil = 60\n",
    "# Load all data and timestamp: Neural-Pace (NP) signal, GoPro videos, Pupil videos, Xsens, phone (acc, gyro, mag, GPS, light, audio)\n",
    "load_syncronized_folder = f'../../RW{subject}/RW{subject}-Walk{walk}-extracted/'\n",
    "save_syncronized_splt_folder = f'../../synchronized/RW{subject}/RW{subject}-Walk{walk}-self-syncronize-split/'\n",
    "if not os.path.exists(save_syncronized_splt_folder):\n",
    "    # Create the directory\n",
    "    os.makedirs(save_syncronized_splt_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chest acc: (98363,) (98363, 3)\n",
      "chest gyro: (55152,) (55152, 3)\n",
      "chest mag: (53540,) (53540, 3)\n",
      "chest light: (4323,) (4323, 1)\n",
      "chest gps: (142,) (142, 2)\n",
      "pupil acc: (87262,) (87262, 3)\n",
      "pupil gyro: (50581,) (50581, 3)\n",
      "pupil mag: (51420,) (51420, 3)\n",
      "pupil gps: (125,) (125, 2)\n",
      "xs: (134445,) (134445, 9) 2021-04-27_12-03-56-081 2021-04-27_12-26-20-531\n"
     ]
    }
   ],
   "source": [
    "# Event Description PupilFrame  GoProFrame  NPSample    NTP\n",
    "df = pd.read_csv(f'../../label_RWNApp_Output_Jan2024/evnts_RWNApp_RW{subject}_Walk{walk}.csv')\n",
    "label = df['Event']\n",
    "ntp_label = df['NTP']\n",
    "# PupilFrame = df['PupilFrame']\n",
    "GoProFrame = df['GoProFrame']\n",
    "NPSample = df['NPSample']\n",
    "\n",
    "# convert matlab NTP time to datetime timestamp\n",
    "time_label = [matlab_datenum_to_formatted_string(md) for md in ntp_label/60/60/24]\n",
    "# np.save(save_folder + \"time_label\", time_label)\n",
    "num_of_label = len(time_label)\n",
    "\n",
    "\n",
    "# Neural-Pace (NP) signal, GoPro videos, Pupil videos, Xsens, phone (acc, gyro, mag, GPS, light, audio)\n",
    "# fre_np = 250, fre_gopro = 60, fre_pupil = 60\n",
    "\n",
    "\n",
    "## load np data\n",
    "data_np = np.load(load_syncronized_folder + 'data_np.npy')\n",
    "time_np = np.load(load_syncronized_folder + 'time_np.npy')\n",
    "\n",
    "## load Gopro video\n",
    "data_gopro = load_syncronized_folder + 'data_video_gopro.mp4'\n",
    "time_gopro = np.load(load_syncronized_folder + 'time_gopro.npy')\n",
    "\n",
    "## load Pupil video\n",
    "data_pupil = load_syncronized_folder + 'data_video_pupil.mp4'\n",
    "time_pupil = np.load(load_syncronized_folder + 'time_pupil.npy')\n",
    "\n",
    "## load xsense data: only center of mass currently\n",
    "df_xsense = pd.read_csv(load_syncronized_folder + 'data_xs_Center-of-Mass.csv')\n",
    "    \n",
    "time_xsense = np.load(load_syncronized_folder + 'time_xs.npy')\n",
    "# frame_xsense = df_xsense['Frame']\n",
    "data_xsense = df_xsense.iloc[:, 1:]\n",
    "\n",
    "## load all of chestphone data\n",
    "df = pd.read_csv(load_syncronized_folder + \"data_chest_phone_acc.csv\", header=None)\n",
    "time_chestphone_acc = df.iloc[:, 0]\n",
    "data_chestphone_acc = df.iloc[:, 1:]\n",
    "print(\"chest acc:\", time_chestphone_acc.shape, data_chestphone_acc.shape)\n",
    "\n",
    "df = pd.read_csv(load_syncronized_folder + \"data_chest_phone_gyro.csv\", header=None)\n",
    "time_chestphone_gyro = df.iloc[:, 0]\n",
    "data_chestphone_gyro = df.iloc[:, 1:]\n",
    "print(\"chest gyro:\", time_chestphone_gyro.shape, data_chestphone_gyro.shape)\n",
    "\n",
    "df = pd.read_csv(load_syncronized_folder + \"data_chest_phone_mag.csv\", header=None)\n",
    "time_chestphone_mag = df.iloc[:, 0]\n",
    "data_chestphone_mag = df.iloc[:, 1:]\n",
    "print(\"chest mag:\", time_chestphone_mag.shape, data_chestphone_mag.shape)\n",
    "\n",
    "df = pd.read_csv(load_syncronized_folder + \"data_chest_phone_light.csv\", header=None)\n",
    "time_chestphone_light = df.iloc[:, 0]\n",
    "data_chestphone_light = df.iloc[:, 1:]\n",
    "print(\"chest light:\", time_chestphone_light.shape, data_chestphone_light.shape)\n",
    "\n",
    "df = pd.read_csv(load_syncronized_folder + \"data_chest_phone_gps.csv\", header=None)\n",
    "time_chestphone_gps = df.iloc[:, 0]\n",
    "data_chestphone_gps_1 = df.iloc[:, 1].str.extract(r'Lat: (\\d+\\.\\d+)', expand=False)## delete the str of \"Lat:\"\n",
    "data_chestphone_gps_1 = np.array(data_chestphone_gps_1.astype(float))\n",
    "data_chestphone_gps_2 = df.iloc[:, 2].str.extract(r'Long: (-?\\d+\\.\\d+)', expand=False)## delete the str of \"Long:\"\n",
    "data_chestphone_gps_2 = np.array(data_chestphone_gps_2.astype(float))\n",
    "data_chestphone_gps = np.stack((data_chestphone_gps_1, data_chestphone_gps_2), axis=1)\n",
    "print(\"chest gps:\", time_chestphone_gps.shape, data_chestphone_gps.shape)\n",
    "\n",
    "\n",
    "## load all of pupilphone data\n",
    "df = pd.read_csv(load_syncronized_folder + \"data_pupil_phone_acc.csv\", header=None)\n",
    "time_pupilphone_acc = df.iloc[:, 0]\n",
    "data_pupilphone_acc = df.iloc[:, 1:]\n",
    "print(\"pupil acc:\", time_pupilphone_acc.shape, data_pupilphone_acc.shape)\n",
    "\n",
    "df = pd.read_csv(load_syncronized_folder + \"data_pupil_phone_gyro.csv\", header=None)\n",
    "time_pupilphone_gyro = df.iloc[:, 0]\n",
    "data_pupilphone_gyro = df.iloc[:, 1:]\n",
    "print(\"pupil gyro:\", time_pupilphone_gyro.shape, data_pupilphone_gyro.shape)\n",
    "\n",
    "df = pd.read_csv(load_syncronized_folder + \"data_pupil_phone_mag.csv\", header=None)\n",
    "time_pupilphone_mag = df.iloc[:, 0]\n",
    "data_pupilphone_mag = df.iloc[:, 1:]\n",
    "print(\"pupil mag:\", time_pupilphone_mag.shape, data_pupilphone_mag.shape)\n",
    "\n",
    "df = pd.read_csv(load_syncronized_folder + \"data_pupil_phone_gps.csv\", header=None)\n",
    "time_pupilphone_gps = df.iloc[:, 0]\n",
    "data_pupilphone_gps_1 = df.iloc[:, 1].str.extract(r'Lat: (/d+/./d+)', expand=False)## delete the str of \"Lat:\"\n",
    "data_pupilphone_gps_1 = np.array(data_pupilphone_gps_1.astype(float))\n",
    "data_pupilphone_gps_2 = df.iloc[:, 2].str.extract(r'Long: (-?/d+/./d+)', expand=False)## delete the str of \"Long:\"\n",
    "data_pupilphone_gps_2 = np.array(data_pupilphone_gps_2.astype(float))\n",
    "data_pupilphone_gps = np.stack((data_pupilphone_gps_1, data_pupilphone_gps_2), axis=1)\n",
    "print(\"pupil gps:\", time_pupilphone_gps.shape, data_pupilphone_gps.shape)\n",
    "\n",
    "print(\"xs:\", time_xsense.shape, data_xsense.shape, time_xsense[0], time_xsense[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Merge gps data and time in the same second\n",
    "## Chest phone\n",
    "time = time_chestphone_gps[0]\n",
    "gps_datas = []\n",
    "gps_times = []\n",
    "gps_data = []\n",
    "for idx in range(len(time_chestphone_gps)):\n",
    "    if time_chestphone_gps[idx][:19] == time[:19]:\n",
    "        gps_data.append(data_chestphone_gps[idx])\n",
    "    else:\n",
    "        gps_datas.append(np.mean(gps_data, axis=0))\n",
    "        # print(idx, gps_datas)\n",
    "        gps_times.append(time)\n",
    "        gps_data.clear()\n",
    "        time = time_chestphone_gps[idx]\n",
    "        gps_data.append(data_chestphone_gps[idx])\n",
    "\n",
    "time_chestphone_gps = pd.Series(gps_times)\n",
    "data_chestphone_gps = np.array(gps_datas)\n",
    "\n",
    "## Pupil phone\n",
    "time = time_pupilphone_gps[0]\n",
    "gps_datas = []\n",
    "gps_times = []\n",
    "gps_data = []\n",
    "for idx in range(len(time_pupilphone_gps)):\n",
    "    if time_pupilphone_gps[idx][:19] == time[:19]:\n",
    "        gps_data.append(data_pupilphone_gps[idx])\n",
    "    else:\n",
    "        gps_datas.append(np.mean(gps_data, axis=0))\n",
    "        # print(idx, gps_datas)\n",
    "        gps_times.append(time)\n",
    "        gps_data.clear()\n",
    "        time = time_pupilphone_gps[idx]\n",
    "        gps_data.append(data_pupilphone_gps[idx])\n",
    "\n",
    "time_pupilphone_gps = pd.Series(gps_times)\n",
    "data_pupilphone_gps = np.array(gps_datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Frames\n",
    "video_clip = VideoFileClip(data_gopro)\n",
    "# ntp_start_0 = ntp_label[0] - GoProFrame[0] / video_clip.fps\n",
    "# short_frames = [22]\n",
    "greped_index = {} # {greped_index: (greped_label, grep reason, time label)} indexs that got filtered\n",
    "label_total_time = {} # {label: datetime.timedelta} total time in one walk of each label\n",
    "label_missing_cnt = {} # {label: missing times}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling frequency:\n",
      "pupil_acc_sample_freq: 10000.0\n",
      "pupil_gps_sample_freq: 0.0\n",
      "pupil_gyro_sample_freq: 20000.0\n",
      "pupil_mag_sample_freq: 20000.0\n",
      "chest_acc_sample_freq: 10000.0\n",
      "chest_gps_sample_freq: 0.0\n",
      "chest_gyro_sample_freq: 19000.0\n",
      "chest_mag_sample_freq: 20000.0\n",
      "chest_light_sample_freq: 98000.0\n"
     ]
    }
   ],
   "source": [
    "def get_sampling_freq(time_series):\n",
    "    time_series = pd.to_datetime(time_series, format='%Y-%m-%d_%H-%M-%S-%f')\n",
    "    time_differences = time_series.diff()\n",
    "    time_differences = time_differences.dropna()\n",
    "    time_differences = time_differences.dt.total_seconds()*1e6\n",
    "    # print(time_differences)\n",
    "    sample_freq = time_differences.mode()\n",
    "    return sample_freq.iloc[0]\n",
    "\n",
    "\n",
    "pupil_acc_sample_freq = get_sampling_freq(time_pupilphone_acc)\n",
    "pupil_gps_sample_freq = get_sampling_freq(time_pupilphone_gps)\n",
    "pupil_gyro_sample_freq = get_sampling_freq(time_pupilphone_gyro)\n",
    "pupil_mag_sample_freq = get_sampling_freq(time_pupilphone_mag)\n",
    "chest_acc_sample_freq = get_sampling_freq(time_chestphone_acc)\n",
    "chest_gps_sample_freq = get_sampling_freq(time_chestphone_gps)\n",
    "chest_gyro_sample_freq = get_sampling_freq(time_chestphone_gyro)\n",
    "chest_mag_sample_freq = get_sampling_freq(time_chestphone_mag)\n",
    "chest_light_sample_freq = get_sampling_freq(time_chestphone_light)\n",
    "\n",
    "print(\"Sampling frequency:\")\n",
    "print(\"pupil_acc_sample_freq:\", pupil_acc_sample_freq)\n",
    "print(\"pupil_gps_sample_freq:\", pupil_gps_sample_freq) # The most frequent difference is 0.0 but not exactly. Manually set to 7 seconds\n",
    "print(\"pupil_gyro_sample_freq:\", pupil_gyro_sample_freq)\n",
    "print(\"pupil_mag_sample_freq:\", pupil_mag_sample_freq)\n",
    "print(\"chest_acc_sample_freq:\", chest_acc_sample_freq)\n",
    "print(\"chest_gps_sample_freq:\", chest_gps_sample_freq) # The most frequent difference is 0.0 but not exactly. Manually set to 7 seconds\n",
    "print(\"chest_gyro_sample_freq:\", chest_gyro_sample_freq)\n",
    "print(\"chest_mag_sample_freq:\", chest_mag_sample_freq)\n",
    "print(\"chest_light_sample_freq:\", chest_light_sample_freq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.typing import NDArray\n",
    "import datetime\n",
    "\n",
    "sample_freq_dict = {\n",
    "    'chestphone_gps': 7,\n",
    "    'chestphone_light': 0.1,\n",
    "    'pupilphone_gps': 7\n",
    "} #unit: second\n",
    "\n",
    "def calculate_time_diff(time_str1, time_str2):\n",
    "    '''Calculate the difference between time_str1 and time_str2. Both input times are str in format: '%Y-%m-%d_%H-%M-%S-%f' '''\n",
    "    time1 = datetime.datetime.strptime(time_str1, '%Y-%m-%d_%H-%M-%S-%f')\n",
    "    time2 = datetime.datetime.strptime(time_str2, '%Y-%m-%d_%H-%M-%S-%f')\n",
    "\n",
    "    return abs(time2 - time1).total_seconds()\n",
    "\n",
    "def extract_modalities(\n",
    "        start_frame_index : int,\n",
    "        end_frame_index : int,\n",
    "        time_arr : NDArray,\n",
    "        data_arr : NDArray,\n",
    "        event : str, # all events except for gopro. Gopro is processed outside this function.\n",
    "        extract_np : bool = True, # whether to extract and save the np file. Default is True. If setting to False, this function only returns whether the modality is missing or not.\n",
    "        time_label = time_label,\n",
    "        save_syncronized_splt_folder : str = save_syncronized_splt_folder,\n",
    "    ):\n",
    "    global label_missing_cnt, modalit_missing_time\n",
    "    start_frame = find_close_frame(time_label[start_frame_index], time_arr)\n",
    "    end_frame = find_close_frame(time_label[end_frame_index], time_arr)\n",
    "    dura = calculate_duration(time_label[start_frame_index], time_label[end_frame_index])\n",
    "\n",
    "    print(\"time_{}: \".format(event), time_arr[start_frame], time_arr[end_frame], calculate_duration(time_arr[start_frame], time_arr[end_frame]))\n",
    "    \n",
    "    if event in ['chestphone_acc', 'chestphone_gyro', 'chestphone_mag', 'pupilphone_acc', 'pupilphone_gyro', 'pupilphone_mag', 'xs_CoM']:\n",
    "        if end_frame <= start_frame:\n",
    "            label_missing_cnt[event] = label_missing_cnt.get(event, 0) + 1\n",
    "            return False\n",
    "        else:\n",
    "            if extract_np:\n",
    "                output_path = save_syncronized_splt_folder + '{}_{}.npy'.format(start_frame_index, event)\n",
    "                np.save(output_path, data_arr[start_frame:end_frame])\n",
    "            return True\n",
    "    elif event in ['chestphone_gps', 'chestphone_light', 'pupilphone_gps']:\n",
    "        if end_frame <= start_frame:\n",
    "            if calculate_time_diff(time_label[start_frame_index], time_arr[start_frame]) > sample_freq_dict[event]:\n",
    "                label_missing_cnt[event] = label_missing_cnt.get(event, 0) + 1\n",
    "                return False\n",
    "            else:\n",
    "                end_frame += 1\n",
    "\n",
    "        # If end_frame > start_frame or time_diff <= sample_freq, return true\n",
    "        if extract_np:\n",
    "            output_path = save_syncronized_splt_folder + '{}_{}.npy'.format(start_frame_index, event)\n",
    "            np.save(output_path, data_arr[start_frame:end_frame])\n",
    "        \n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame  65 :\n",
      "label:  Doorway\n",
      "End frame index: 66\n",
      "time_label:  2021-04-28_10-08-26-064 2021-04-28_10-08-26-081 0:00:00.017000\n",
      "time_gopro:  2021-04-28_10-08-26-073 2021-04-28_10-08-26-089 0:00:00.016000\n",
      "time_np:  2021-04-28_10-08-26-069 2021-04-28_10-08-26-081 0:00:00.012000 (4, 4)\n",
      "subset_clip time: 1293.3587333333332 1293.4254666666666\n",
      "start frame, end frame: 38762 38764\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "t_start (1293.36) should be smaller than the clip's duration (1126.31).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 93\u001b[0m\n\u001b[0;32m     91\u001b[0m output_path \u001b[38;5;241m=\u001b[39m save_syncronized_splt_folder \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_gopro.mp4\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(start_frame_index)\n\u001b[0;32m     92\u001b[0m output_path_audio \u001b[38;5;241m=\u001b[39m save_syncronized_splt_folder \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_gopro_audio.wav\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(start_frame_index)\n\u001b[1;32m---> 93\u001b[0m \u001b[43mextract_video_audio_subset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_gopro\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_frame_gopro\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_frame_gopro\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_path_audio\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m modality_num \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;66;03m# will extract two modalities\u001b[39;00m\n\u001b[0;32m     96\u001b[0m output_path \u001b[38;5;241m=\u001b[39m save_syncronized_splt_folder \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_pupil.mp4\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(start_frame_index)\n",
      "Cell \u001b[1;32mIn[35], line 84\u001b[0m, in \u001b[0;36mextract_video_audio_subset\u001b[1;34m(video_path, start_frame, end_frame, output_video_path, output_audio_path)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msubset_clip time:\u001b[39m\u001b[38;5;124m\"\u001b[39m, start_frame \u001b[38;5;241m/\u001b[39m video_clip\u001b[38;5;241m.\u001b[39mfps, end_frame \u001b[38;5;241m/\u001b[39m video_clip\u001b[38;5;241m.\u001b[39mfps)\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart frame, end frame:\u001b[39m\u001b[38;5;124m\"\u001b[39m, start_frame, end_frame)\n\u001b[1;32m---> 84\u001b[0m subset_clip \u001b[38;5;241m=\u001b[39m \u001b[43mvideo_clip\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubclip\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_frame\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mvideo_clip\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_frame\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mvideo_clip\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;66;03m# Write video file\u001b[39;00m\n\u001b[0;32m     87\u001b[0m subset_clip\u001b[38;5;241m.\u001b[39mwrite_videofile(output_video_path, codec\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibx264\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32md:\\Users\\14815\\miniconda3\\envs\\multimoda\\lib\\site-packages\\decorator.py:232\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwsyntax:\n\u001b[0;32m    231\u001b[0m     args, kw \u001b[38;5;241m=\u001b[39m fix(args, kw, sig)\n\u001b[1;32m--> 232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m caller(func, \u001b[38;5;241m*\u001b[39m(extras \u001b[38;5;241m+\u001b[39m args), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[1;32md:\\Users\\14815\\miniconda3\\envs\\multimoda\\lib\\site-packages\\moviepy\\decorators.py:84\u001b[0m, in \u001b[0;36mpreprocess_args.<locals>.wrapper\u001b[1;34m(f, *a, **kw)\u001b[0m\n\u001b[0;32m     80\u001b[0m new_a \u001b[38;5;241m=\u001b[39m [fun(arg) \u001b[38;5;28;01mif\u001b[39;00m (name \u001b[38;5;129;01min\u001b[39;00m varnames) \u001b[38;5;28;01melse\u001b[39;00m arg\n\u001b[0;32m     81\u001b[0m          \u001b[38;5;28;01mfor\u001b[39;00m (arg, name) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(a, names)]\n\u001b[0;32m     82\u001b[0m new_kw \u001b[38;5;241m=\u001b[39m {k: fun(v) \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m varnames \u001b[38;5;28;01melse\u001b[39;00m v\n\u001b[0;32m     83\u001b[0m          \u001b[38;5;28;01mfor\u001b[39;00m (k,v) \u001b[38;5;129;01min\u001b[39;00m kw\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m---> 84\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39mnew_a, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mnew_kw)\n",
      "File \u001b[1;32md:\\Users\\14815\\miniconda3\\envs\\multimoda\\lib\\site-packages\\decorator.py:232\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwsyntax:\n\u001b[0;32m    231\u001b[0m     args, kw \u001b[38;5;241m=\u001b[39m fix(args, kw, sig)\n\u001b[1;32m--> 232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m caller(func, \u001b[38;5;241m*\u001b[39m(extras \u001b[38;5;241m+\u001b[39m args), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[1;32md:\\Users\\14815\\miniconda3\\envs\\multimoda\\lib\\site-packages\\moviepy\\decorators.py:29\u001b[0m, in \u001b[0;36mapply_to_mask\u001b[1;34m(f, clip, *a, **k)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;129m@decorator\u001b[39m\u001b[38;5;241m.\u001b[39mdecorator\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_to_mask\u001b[39m(f, clip, \u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mk):\n\u001b[0;32m     26\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" This decorator will apply the same function f to the mask of\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;124;03m        the clip created with f \"\"\"\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m     newclip \u001b[38;5;241m=\u001b[39m f(clip, \u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mk)\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(newclip, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmask\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     31\u001b[0m         newclip\u001b[38;5;241m.\u001b[39mmask \u001b[38;5;241m=\u001b[39m f(newclip\u001b[38;5;241m.\u001b[39mmask, \u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mk)\n",
      "File \u001b[1;32md:\\Users\\14815\\miniconda3\\envs\\multimoda\\lib\\site-packages\\decorator.py:232\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwsyntax:\n\u001b[0;32m    231\u001b[0m     args, kw \u001b[38;5;241m=\u001b[39m fix(args, kw, sig)\n\u001b[1;32m--> 232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m caller(func, \u001b[38;5;241m*\u001b[39m(extras \u001b[38;5;241m+\u001b[39m args), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[1;32md:\\Users\\14815\\miniconda3\\envs\\multimoda\\lib\\site-packages\\moviepy\\decorators.py:41\u001b[0m, in \u001b[0;36mapply_to_audio\u001b[1;34m(f, clip, *a, **k)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;129m@decorator\u001b[39m\u001b[38;5;241m.\u001b[39mdecorator\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_to_audio\u001b[39m(f, clip, \u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mk):\n\u001b[0;32m     38\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" This decorator will apply the function f to the audio of\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;124;03m        the clip created with f \"\"\"\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m     newclip \u001b[38;5;241m=\u001b[39m f(clip, \u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mk)\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(newclip, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maudio\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     43\u001b[0m         newclip\u001b[38;5;241m.\u001b[39maudio \u001b[38;5;241m=\u001b[39m f(newclip\u001b[38;5;241m.\u001b[39maudio, \u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mk)\n",
      "File \u001b[1;32md:\\Users\\14815\\miniconda3\\envs\\multimoda\\lib\\site-packages\\moviepy\\Clip.py:385\u001b[0m, in \u001b[0;36mClip.subclip\u001b[1;34m(self, t_start, t_end)\u001b[0m\n\u001b[0;32m    382\u001b[0m     t_start \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mduration \u001b[38;5;241m+\u001b[39m t_start   \u001b[38;5;66;03m# Remember t_start is negative\u001b[39;00m\n\u001b[0;32m    384\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mduration \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (t_start \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mduration):\n\u001b[1;32m--> 385\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mt_start (\u001b[39m\u001b[38;5;132;01m%.02f\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m t_start \u001b[38;5;241m+\u001b[39m\n\u001b[0;32m    386\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshould be smaller than the clip\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m\n\u001b[0;32m    387\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mduration (\u001b[39m\u001b[38;5;132;01m%.02f\u001b[39;00m\u001b[38;5;124m).\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mduration)\n\u001b[0;32m    389\u001b[0m newclip \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfl_time(\u001b[38;5;28;01mlambda\u001b[39;00m t: t \u001b[38;5;241m+\u001b[39m t_start, apply_to\u001b[38;5;241m=\u001b[39m[])\n\u001b[0;32m    391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (t_end \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mduration \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n",
      "\u001b[1;31mValueError\u001b[0m: t_start (1293.36) should be smaller than the clip's duration (1126.31)."
     ]
    }
   ],
   "source": [
    "import csv\n",
    "## Start at \"Walk Beg\"\n",
    "walk_beg_index = 0\n",
    "func_extract_label = lambda index : label[index][:-4] if (label[index].endswith(\"Beg\") or label[index].endswith(\"End\")) else label[index]\n",
    "while label[walk_beg_index] != \"Walk Beg\":\n",
    "    greped_index[walk_beg_index] = (func_extract_label(walk_beg_index), \"Event before Walk Beg\", time_label[walk_beg_index])\n",
    "    walk_beg_index += 1\n",
    "\n",
    "file_stats = save_syncronized_splt_folder + \"index_stats.csv\"\n",
    "\n",
    "with open(file_stats, mode='w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    # Write the header row\n",
    "    writer.writerow([\"Index\", \"Label\", \"Duration\", \"Num of Modalities\", \"Missing Modalities\"])\n",
    "    \n",
    "    temp_index = walk_beg_index + 1\n",
    "    while temp_index < num_of_label:\n",
    "        ## End at \"Walk End\"\n",
    "        if label[temp_index] == \"Walk End\":\n",
    "            break\n",
    "        \n",
    "        try:\n",
    "            ## skip \"xxx End\" events\n",
    "            if label[temp_index].endswith(\"End\"):\n",
    "                continue\n",
    "            \n",
    "            start_frame_index = temp_index\n",
    "            end_frame_index = start_frame_index + 1\n",
    "\n",
    "            cut_label = func_extract_label(start_frame_index)\n",
    "\n",
    "            ## grep not interested labels\n",
    "            if cut_label not in [\"Doorway\", \"Talking\", \"Correct Turn\", \"Incorrect Turn\", \"Lost\", \"Stop\", \"Abnormal\", \"Pointing\", \"Outdoor\", \"Choice Point\", \"Stare\", \"New Context\"]:\n",
    "                greped_index[start_frame_index] = (cut_label, \"Not intereted event\", time_label[start_frame_index])\n",
    "                continue\n",
    "\n",
    "            ## Find end_frame_index\n",
    "            if label[start_frame_index].endswith(\"Beg\"):\n",
    "                while end_frame_index < num_of_label:\n",
    "                    if label[end_frame_index].endswith(\"End\") and func_extract_label(end_frame_index) == cut_label:\n",
    "                        break\n",
    "                    end_frame_index += 1\n",
    "                if end_frame_index >= num_of_label:\n",
    "                    raise ValueError(f\"Corresponding End event missing: {start_frame_index} {cut_label}\")\n",
    "            else: # merge successive identical label\n",
    "                while func_extract_label(end_frame_index) == cut_label and end_frame_index < num_of_label:\n",
    "                    end_frame_index += 1\n",
    "                temp_index = end_frame_index - 1 # align with the \"+1\" later\n",
    "            ########################\n",
    "\n",
    "            modality_num = 0\n",
    "            missing_modality = []\n",
    "            extract_np = True\n",
    "\n",
    "            print(\"Frame \", start_frame_index, \":\")\n",
    "            print(\"label: \", label[start_frame_index])\n",
    "            print(\"End frame index:\", end_frame_index)\n",
    "            # input(\"Press to continue......\")\n",
    "            print(\"time_label: \", time_label[start_frame_index], time_label[end_frame_index], calculate_duration(time_label[start_frame_index], time_label[end_frame_index]))\n",
    "\n",
    "            ############### grep invalid data. If one of gopro/pupil/np frame is missing, grep all data ###############\n",
    "            start_frame_gopro = GoProFrame[start_frame_index]\n",
    "            end_frame_gopro = GoProFrame[end_frame_index]\n",
    "            dura_gopro = calculate_duration(time_gopro[start_frame_gopro], time_gopro[end_frame_gopro])\n",
    "            print(\"time_gopro: \", time_gopro[start_frame_gopro], time_gopro[end_frame_gopro], calculate_duration(time_gopro[start_frame_gopro], time_gopro[end_frame_gopro]))\n",
    "\n",
    "            if end_frame_gopro <= start_frame_gopro:\n",
    "                label_missing_cnt['gopro'] = label_missing_cnt.get('gopro', 0) + 1\n",
    "                greped_index[start_frame_index] = (cut_label, 'gopro', time_label[start_frame_index])\n",
    "                extract_np = False\n",
    "                # raise ValueError(\"End frame must be greater than start frame:gopro\")\n",
    "\n",
    "            ############### sample np signals ###############\n",
    "            start_frame_np = NPSample[start_frame_index]\n",
    "            end_frame_np = NPSample[end_frame_index]\n",
    "            print(\"time_np: \", time_np[start_frame_np], time_np[end_frame_np-1], calculate_duration(time_np[start_frame_np], time_np[end_frame_np-1]), data_np[start_frame_np:end_frame_np].shape) # the extracted end_frame_np is started from 1, so need to subtract 1 when apply to time_np. data_np slice won't include end_frame_np. Other modalities don't have this problem.\n",
    "            if end_frame_np <= start_frame_np:\n",
    "                label_missing_cnt['np'] = label_missing_cnt.get('np', 0) + 1\n",
    "                greped_index[start_frame_index] = (cut_label, 'np', time_label[start_frame_index])\n",
    "                extract_np = False\n",
    "            else:\n",
    "                output_path = save_syncronized_splt_folder + '{}_np.npy'.format(start_frame_index)\n",
    "                np.save(output_path, data_np[start_frame_np:end_frame_np])\n",
    "                modality_num += 1\n",
    "\n",
    "            ############### sample gopro videos ###############\n",
    "            if extract_np:\n",
    "                output_path = save_syncronized_splt_folder + '{}_gopro.mp4'.format(start_frame_index)\n",
    "                output_path_audio = save_syncronized_splt_folder + '{}_gopro_audio.wav'.format(start_frame_index)\n",
    "                extract_video_audio_subset(data_gopro, start_frame_gopro, end_frame_gopro, output_path, output_path_audio)\n",
    "                modality_num += 2 # will extract two modalities\n",
    "            \n",
    "                output_path = save_syncronized_splt_folder + '{}_pupil.mp4'.format(start_frame_index)\n",
    "                extract_video_noaudio_subset(data_pupil, start_frame_gopro, end_frame_gopro, output_path)\n",
    "                modality_num += 1\n",
    "\n",
    "            ############### sample other data ###############\n",
    "            if extract_modalities(start_frame_index, end_frame_index, time_chestphone_acc, data_chestphone_acc, event='chestphone_acc', extract_np=extract_np):\n",
    "                modality_num += 1\n",
    "            else:\n",
    "                missing_modality.append('chestphone_acc')\n",
    "            \n",
    "            if extract_modalities(start_frame_index, end_frame_index, time_chestphone_gyro, data_chestphone_gyro, event='chestphone_gyro', extract_np=extract_np):\n",
    "                modality_num += 1\n",
    "            else:\n",
    "                missing_modality.append('chestphone_gyro')\n",
    "            \n",
    "            if extract_modalities(start_frame_index, end_frame_index, time_chestphone_mag, data_chestphone_mag, event='chestphone_mag', extract_np=extract_np):\n",
    "                modality_num += 1\n",
    "            else:\n",
    "                missing_modality.append('chestphone_mag')\n",
    "            \n",
    "            if extract_modalities(start_frame_index, end_frame_index, time_chestphone_gps, data_chestphone_gps, event='chestphone_gps', extract_np=extract_np):\n",
    "                modality_num += 1\n",
    "            else:\n",
    "                missing_modality.append('chestphone_gps')\n",
    "                \n",
    "            if extract_modalities(start_frame_index, end_frame_index, time_chestphone_light, data_chestphone_light, event='chestphone_light', extract_np=extract_np):\n",
    "                modality_num += 1\n",
    "            else:\n",
    "                missing_modality.append('chestphone_light')\n",
    "            \n",
    "            if extract_modalities(start_frame_index, end_frame_index, time_pupilphone_acc, data_pupilphone_acc, event='pupilphone_acc', extract_np=extract_np):\n",
    "                modality_num += 1\n",
    "            else:\n",
    "                missing_modality.append('pupilphone_acc')\n",
    "            \n",
    "            if extract_modalities(start_frame_index, end_frame_index, time_pupilphone_gyro, data_pupilphone_gyro, event='pupilphone_gyro', extract_np=extract_np):\n",
    "                modality_num += 1\n",
    "            else:\n",
    "                missing_modality.append('pupilphone_gyro')\n",
    "            \n",
    "            if extract_modalities(start_frame_index,  end_frame_index, time_pupilphone_mag, data_pupilphone_mag, event='pupilphone_mag', extract_np=extract_np):\n",
    "                modality_num += 1\n",
    "            else:\n",
    "                missing_modality.append('pupilphone_mag')\n",
    "            \n",
    "            if extract_modalities(start_frame_index, end_frame_index, time_pupilphone_gps, data_pupilphone_gps, event='pupilphone_gps', extract_np=extract_np):\n",
    "                modality_num += 1\n",
    "            else:\n",
    "                missing_modality.append('pupilphone_gps')\n",
    "            \n",
    "            if extract_modalities(start_frame_index, end_frame_index, time_xsense, data_xsense, event='xs_CoM', extract_np=extract_np):\n",
    "                modality_num += 1\n",
    "            else:\n",
    "                missing_modality.append('xs_CoM')\n",
    "\n",
    "            ############### record label time ###############\n",
    "            if extract_np:\n",
    "                dura = calculate_duration(time_label[start_frame_index], time_label[end_frame_index])\n",
    "                label_total_time[cut_label] = label_total_time.get(cut_label, datetime.timedelta()) + dura\n",
    "                writer.writerow([start_frame_index, cut_label, dura, modality_num, \", \".join(missing_modality)])\n",
    "            \n",
    "        except Exception as e:\n",
    "            greped_index[start_frame_index] = (cut_label, str(e), time_label[start_frame_index])\n",
    "            print(\"Exception:\", str(e))\n",
    "        finally:\n",
    "            temp_index += 1\n",
    "\n",
    "assert (temp_index == num_of_label or label[temp_index] == \"Walk End\"), f\"Unexpected exit: temp_index={temp_index}\"\n",
    "for i in range(temp_index, num_of_label):\n",
    "    greped_index[i] = (func_extract_label(i), \"Event after Walk End\", time_label[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing counts of missing label......\n",
      "Counts of missing labels have been saved to ../../synchronized_test/RW1/RW1-Walk1-self-syncronize-split/missing_label_cnt.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "# write index label map, greped index and label total time into .csv file\n",
    "file_label_time = save_syncronized_splt_folder + \"cal_label_time.csv\"\n",
    "file_greped = save_syncronized_splt_folder + \"greped_index.csv\"\n",
    "file_miss = save_syncronized_splt_folder + \"missing_label_cnt.csv\"\n",
    "\n",
    "# Writing the dictionary to a CSV file with multiple columns\n",
    "\n",
    "print(\"Writing total time of each label......\")\n",
    "with open(file_label_time, mode='w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    # Write the header row\n",
    "    writer.writerow(['Label', 'Total Time in Walk'])\n",
    "    # Write the dictionary entries\n",
    "    for i_label, i_time in label_total_time.items():\n",
    "        writer.writerow([i_label, str(i_time)])\n",
    "print(f\"Total time of each label has been saved to {file_label_time}\")\n",
    "\n",
    "print(\"Writing greped label......\")\n",
    "with open(file_greped, mode='w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    # Write the header row\n",
    "    writer.writerow(['Index', 'Label', 'Grep Reason', 'Time Label'])\n",
    "    # Write the dictionary entries\n",
    "    for index, value in greped_index.items():\n",
    "        i_label, reason, time_label = value\n",
    "        writer.writerow([index, i_label, reason, time_label])\n",
    "print(f\"Greped label has been saved to {file_greped}\")\n",
    "\n",
    "print(\"Writing counts of label......\")\n",
    "with open(file_miss, mode='w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    # Write the header row\n",
    "    writer.writerow(['Modality', 'Missing Count'])\n",
    "    # Write the dictionary entries\n",
    "    for i_label, cnt in label_missing_cnt.items():\n",
    "        writer.writerow([i_label, cnt])\n",
    "print(f\"Counts of missing labels have been saved to {file_miss}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
